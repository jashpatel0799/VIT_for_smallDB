Fri Aug  9 12:53:29 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:B7:00.0 Off |                    0 |
| N/A   53C    P0             86W /  400W |       1MiB /  40960MiB |     58%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+


Experiment Name: org_architecture
Experiment Details: base vit architecture but patch size 4


Dataset Name: cifar10
Seed: 64
Batch Size: 32
Number of Epochs: 100
Learning Rate: 3e-4
Input Channel: 3
Patch Size: 4
Embedding Size: 48
Input Image Size: 224
ViT Depth: 12
Number of Classes: 10
WandB Project: vit-small-data
WandB Run Name: org_architecture_cifar10_Lr_3e-4_EMB_48_patch_4_depth_12
Output Directory: results


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─PatchEmbedding: 1-1                    [-1, 3137, 48]            --
|    └─Sequential: 2-1                   [-1, 3136, 48]            --
|    |    └─Conv2d: 3-1                  [-1, 48, 56, 56]          2,352
|    |    └─Rearrange: 3-2               [-1, 3136, 48]            --
├─TransformerEncoder: 1-2                [-1, 3137, 48]            --
|    └─TransformerEncoderBlock: 2-2      [-1, 3137, 48]            --
|    |    └─Residual: 3-3                [-1, 3137, 48]            9,504
|    |    └─Residual: 3-4                [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-3      [-1, 3137, 48]            --
|    |    └─Residual: 3-5                [-1, 3137, 48]            9,504
|    |    └─Residual: 3-6                [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-4      [-1, 3137, 48]            --
|    |    └─Residual: 3-7                [-1, 3137, 48]            9,504
|    |    └─Residual: 3-8                [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-5      [-1, 3137, 48]            --
|    |    └─Residual: 3-9                [-1, 3137, 48]            9,504
|    |    └─Residual: 3-10               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-6      [-1, 3137, 48]            --
|    |    └─Residual: 3-11               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-12               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-7      [-1, 3137, 48]            --
|    |    └─Residual: 3-13               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-14               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-8      [-1, 3137, 48]            --
|    |    └─Residual: 3-15               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-16               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-9      [-1, 3137, 48]            --
|    |    └─Residual: 3-17               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-18               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-10     [-1, 3137, 48]            --
|    |    └─Residual: 3-19               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-20               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-11     [-1, 3137, 48]            --
|    |    └─Residual: 3-21               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-22               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-12     [-1, 3137, 48]            --
|    |    └─Residual: 3-23               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-24               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-13     [-1, 3137, 48]            --
|    |    └─Residual: 3-25               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-26               [-1, 3137, 48]            18,768
├─ClassificationHead: 1-3                [-1, 10]                  --
|    └─Reduce: 2-14                      [-1, 48]                  --
|    └─LayerNorm: 2-15                   [-1, 48]                  96
|    └─Linear: 2-16                      [-1, 10]                  490
==========================================================================================
Total params: 342,202
Trainable params: 342,202
Non-trainable params: 0
Total mult-adds (M): 8.56
==========================================================================================
Input size (MB): 0.57
Forward/backward pass size (MB): 1.15
Params size (MB): 1.31
Estimated Total Size (MB): 3.03
==========================================================================================

 ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─PatchEmbedding: 1-1                    [-1, 3137, 48]            --
|    └─Sequential: 2-1                   [-1, 3136, 48]            --
|    |    └─Conv2d: 3-1                  [-1, 48, 56, 56]          2,352
|    |    └─Rearrange: 3-2               [-1, 3136, 48]            --
├─TransformerEncoder: 1-2                [-1, 3137, 48]            --
|    └─TransformerEncoderBlock: 2-2      [-1, 3137, 48]            --
|    |    └─Residual: 3-3                [-1, 3137, 48]            9,504
|    |    └─Residual: 3-4                [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-3      [-1, 3137, 48]            --
|    |    └─Residual: 3-5                [-1, 3137, 48]            9,504
|    |    └─Residual: 3-6                [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-4      [-1, 3137, 48]            --
|    |    └─Residual: 3-7                [-1, 3137, 48]            9,504
|    |    └─Residual: 3-8                [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-5      [-1, 3137, 48]            --
|    |    └─Residual: 3-9                [-1, 3137, 48]            9,504
|    |    └─Residual: 3-10               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-6      [-1, 3137, 48]            --
|    |    └─Residual: 3-11               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-12               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-7      [-1, 3137, 48]            --
|    |    └─Residual: 3-13               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-14               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-8      [-1, 3137, 48]            --
|    |    └─Residual: 3-15               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-16               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-9      [-1, 3137, 48]            --
|    |    └─Residual: 3-17               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-18               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-10     [-1, 3137, 48]            --
|    |    └─Residual: 3-19               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-20               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-11     [-1, 3137, 48]            --
|    |    └─Residual: 3-21               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-22               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-12     [-1, 3137, 48]            --
|    |    └─Residual: 3-23               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-24               [-1, 3137, 48]            18,768
|    └─TransformerEncoderBlock: 2-13     [-1, 3137, 48]            --
|    |    └─Residual: 3-25               [-1, 3137, 48]            9,504
|    |    └─Residual: 3-26               [-1, 3137, 48]            18,768
├─ClassificationHead: 1-3                [-1, 10]                  --
|    └─Reduce: 2-14                      [-1, 48]                  --
|    └─LayerNorm: 2-15                   [-1, 48]                  96
|    └─Linear: 2-16                      [-1, 10]                  490
==========================================================================================
Total params: 342,202
Trainable params: 342,202
Non-trainable params: 0
Total mult-adds (M): 8.56
==========================================================================================
Input size (MB): 0.57
Forward/backward pass size (MB): 1.15
Params size (MB): 1.31
Estimated Total Size (MB): 3.03
========================================================================================== 

With CIFAR10
Files already downloaded and verified
Files already downloaded and verified


EXP org_architecture: Original VIT on cifar10 with depth 12 and LEARNIGN_RATE 0.0003



wandb: Currently logged in as: jashpatel8561 (maa_64). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /scratch/data/m22cs061/VIT_for_smallDB/ORG/wandb/run-20240809_125349-n2147tfy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run org_architecture_cifar10_Lr_3e-4_EMB_48_patch_4_depth_12
wandb: ⭐️ View project at https://wandb.ai/maa_64/vit-small-data
wandb: 🚀 View run at https://wandb.ai/maa_64/vit-small-data/runs/n2147tfy
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/scratch/data/m22cs061/VIT_for_smallDB/ORG/main.py", line 135, in <module>
    main(config)
  File "/scratch/data/m22cs061/VIT_for_smallDB/ORG/main.py", line 105, in main
    train_model, train_loss, test_loss, train_acc, test_acc = engine.train(model = vit_model,
  File "/scratch/data/m22cs061/VIT_for_smallDB/ORG/engine.py", line 150, in train
    train_loss, train_acc, train_model = train_loop(model = model, dataloader = train_dataloader,
  File "/scratch/data/m22cs061/VIT_for_smallDB/ORG/engine.py", line 26, in train_loop
    pred = model(x_train)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/data/m22cs061/VIT_for_smallDB/ORG/model.py", line 116, in forward
    x = self.fn(x, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/data/m22cs061/VIT_for_smallDB/ORG/model.py", line 97, in forward
    att = self.att_dropout(att)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/iitjhome/m22cs061/miniconda3/envs/vitsmall/lib/python3.10/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.39 GiB. GPU 
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.027 MB of 0.034 MB uploadedwandb: | 0.027 MB of 0.038 MB uploadedwandb: / 0.038 MB of 0.038 MB uploadedwandb: - 0.038 MB of 0.038 MB uploadedwandb: 🚀 View run org_architecture_cifar10_Lr_3e-4_EMB_48_patch_4_depth_12 at: https://wandb.ai/maa_64/vit-small-data/runs/n2147tfy
wandb: ⭐️ View project at: https://wandb.ai/maa_64/vit-small-data
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240809_125349-n2147tfy/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
