Mon Sep 30 21:35:41 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:0F:00.0 Off |                    0 |
| N/A   27C    P0             51W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+


Experiment Name: org_architecture
Experiment Model Number: 2
1. base vit architecture 
2. base vit architecture with RMS norm
Experiment Details: base vit architecture with RSM norm


Dataset Name: cifar100
Seed: 64
Batch Size: 64
Number of Epochs: 100
Learning Rate: 3e-4
Input Channel: 3
Patch Size: 16
Embedding Size: 768
Input Image Size: 224
ViT Depth: 12
Number of Classes: 100
WandB Project: vit-small-data
WandB Run Name: org_architecture_cifar100_Lr_3e-4_EMB_768_patch_16_depth_12
Output Directory: results


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
â”œâ”€PatchEmbedding: 1-1                    [-1, 197, 768]            --
|    â””â”€Sequential: 2-1                   [-1, 196, 768]            --
|    |    â””â”€Conv2d: 3-1                  [-1, 768, 14, 14]         590,592
|    |    â””â”€Rearrange: 3-2               [-1, 196, 768]            --
â”œâ”€TransformerEncoder: 1-2                [-1, 197, 768]            --
|    â””â”€TransformerEncoderBlock: 2-2      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-3                [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-4                [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-3      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-5                [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-6                [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-4      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-7                [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-8                [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-5      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-9                [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-10               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-6      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-11               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-12               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-7      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-13               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-14               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-8      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-15               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-16               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-9      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-17               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-18               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-10     [-1, 197, 768]            --
|    |    â””â”€Residual: 3-19               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-20               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-11     [-1, 197, 768]            --
|    |    â””â”€Residual: 3-21               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-22               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-12     [-1, 197, 768]            --
|    |    â””â”€Residual: 3-23               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-24               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-13     [-1, 197, 768]            --
|    |    â””â”€Residual: 3-25               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-26               [-1, 197, 768]            4,723,968
â”œâ”€ClassificationHead: 1-3                [-1, 100]                 --
|    â””â”€Reduce: 2-14                      [-1, 768]                 --
|    â””â”€LayerNorm: 2-15                   [-1, 768]                 1,536
|    â””â”€Linear: 2-16                      [-1, 100]                 76,900
==========================================================================================
Total params: 85,723,492
Trainable params: 85,723,492
Non-trainable params: 0
Total mult-adds (M): 456.75
==========================================================================================
Input size (MB): 0.57
Forward/backward pass size (MB): 1.16
Params size (MB): 327.01
Estimated Total Size (MB): 328.74
==========================================================================================

 ==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
â”œâ”€PatchEmbedding: 1-1                    [-1, 197, 768]            --
|    â””â”€Sequential: 2-1                   [-1, 196, 768]            --
|    |    â””â”€Conv2d: 3-1                  [-1, 768, 14, 14]         590,592
|    |    â””â”€Rearrange: 3-2               [-1, 196, 768]            --
â”œâ”€TransformerEncoder: 1-2                [-1, 197, 768]            --
|    â””â”€TransformerEncoderBlock: 2-2      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-3                [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-4                [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-3      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-5                [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-6                [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-4      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-7                [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-8                [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-5      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-9                [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-10               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-6      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-11               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-12               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-7      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-13               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-14               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-8      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-15               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-16               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-9      [-1, 197, 768]            --
|    |    â””â”€Residual: 3-17               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-18               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-10     [-1, 197, 768]            --
|    |    â””â”€Residual: 3-19               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-20               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-11     [-1, 197, 768]            --
|    |    â””â”€Residual: 3-21               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-22               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-12     [-1, 197, 768]            --
|    |    â””â”€Residual: 3-23               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-24               [-1, 197, 768]            4,723,968
|    â””â”€TransformerEncoderBlock: 2-13     [-1, 197, 768]            --
|    |    â””â”€Residual: 3-25               [-1, 197, 768]            2,363,904
|    |    â””â”€Residual: 3-26               [-1, 197, 768]            4,723,968
â”œâ”€ClassificationHead: 1-3                [-1, 100]                 --
|    â””â”€Reduce: 2-14                      [-1, 768]                 --
|    â””â”€LayerNorm: 2-15                   [-1, 768]                 1,536
|    â””â”€Linear: 2-16                      [-1, 100]                 76,900
==========================================================================================
Total params: 85,723,492
Trainable params: 85,723,492
Non-trainable params: 0
Total mult-adds (M): 456.75
==========================================================================================
Input size (MB): 0.57
Forward/backward pass size (MB): 1.16
Params size (MB): 327.01
Estimated Total Size (MB): 328.74
========================================================================================== 



EXP org_architecture: Original VIT on cifar100 with depth 12 and LEARNIGN_RATE 0.0003 with model architecture number 2.



With CIFAR100
Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar100/cifar-100-python.tar.gz
  0%|          | 0/169001437 [00:00<?, ?it/s]  0%|          | 32768/169001437 [00:00<23:30, 119814.38it/s]  0%|          | 65536/169001437 [00:00<23:24, 120242.18it/s]  0%|          | 98304/169001437 [00:00<23:05, 121925.31it/s]  0%|          | 229376/169001437 [00:01<10:35, 265461.00it/s]  0%|          | 458752/169001437 [00:01<05:54, 475969.79it/s]  1%|          | 917504/169001437 [00:01<03:08, 891725.53it/s]  1%|          | 1835008/169001437 [00:01<01:37, 1709806.95it/s]  2%|â–         | 3702784/169001437 [00:02<00:49, 3363051.61it/s]  4%|â–Ž         | 6324224/169001437 [00:02<00:30, 5330013.51it/s]  6%|â–Œ         | 9469952/169001437 [00:02<00:21, 7264142.97it/s]  7%|â–‹         | 12582912/169001437 [00:02<00:18, 8459616.79it/s]  9%|â–‰         | 15695872/169001437 [00:03<00:16, 9332090.83it/s] 11%|â–ˆ         | 18808832/169001437 [00:03<00:15, 9917533.65it/s] 13%|â–ˆâ–Ž        | 21954560/169001437 [00:03<00:14, 10414978.55it/s] 15%|â–ˆâ–        | 25100288/169001437 [00:04<00:13, 10761711.16it/s] 17%|â–ˆâ–‹        | 28213248/169001437 [00:04<00:12, 11006670.68it/s] 19%|â–ˆâ–Š        | 31326208/169001437 [00:04<00:12, 10909060.32it/s] 20%|â–ˆâ–ˆ        | 34340864/169001437 [00:04<00:12, 10920540.11it/s] 22%|â–ˆâ–ˆâ–       | 37388288/169001437 [00:05<00:11, 10979446.95it/s] 24%|â–ˆâ–ˆâ–       | 40501248/169001437 [00:05<00:11, 11144281.25it/s] 26%|â–ˆâ–ˆâ–Œ       | 43646976/169001437 [00:05<00:11, 11196783.43it/s] 28%|â–ˆâ–ˆâ–Š       | 46759936/169001437 [00:06<00:10, 11297968.69it/s] 30%|â–ˆâ–ˆâ–‰       | 49905664/169001437 [00:06<00:10, 11373639.91it/s] 31%|â–ˆâ–ˆâ–ˆâ–      | 53018624/169001437 [00:06<00:10, 11528844.93it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 56164352/169001437 [00:06<00:09, 11449855.90it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 59277312/169001437 [00:07<00:09, 11476130.75it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 62423040/169001437 [00:07<00:09, 11511268.96it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 65536000/169001437 [00:07<00:08, 11514089.09it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 68616192/169001437 [00:07<00:09, 10892446.37it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 71663616/169001437 [00:08<00:08, 10952160.55it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 74776576/169001437 [00:08<00:08, 11054673.65it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 77922304/169001437 [00:08<00:08, 11208651.65it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 81035264/169001437 [00:09<00:07, 11163916.63it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 84115456/169001437 [00:09<00:07, 11162511.25it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 87162880/169001437 [00:09<00:07, 11146610.13it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 90243072/169001437 [00:09<00:07, 11155900.07it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 93356032/169001437 [00:10<00:06, 11181380.85it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 96436224/169001437 [00:10<00:06, 11192564.01it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 99549184/169001437 [00:10<00:06, 11222096.84it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 102629376/169001437 [00:10<00:05, 11204923.99it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 105676800/169001437 [00:11<00:05, 11149325.63it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 108724224/169001437 [00:11<00:05, 11135142.55it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 111869952/169001437 [00:11<00:05, 11183346.47it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 114917376/169001437 [00:12<00:04, 11154112.47it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 117932032/169001437 [00:12<00:04, 11080103.11it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 120979456/169001437 [00:12<00:04, 11079670.28it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 124059648/169001437 [00:12<00:04, 11076363.47it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 127041536/169001437 [00:13<00:03, 11004387.22it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 130154496/169001437 [00:13<00:03, 11089411.11it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 133169152/169001437 [00:13<00:03, 11066807.09it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 136216576/169001437 [00:14<00:02, 11070857.18it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 139296768/169001437 [00:14<00:02, 11108883.06it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 142344192/169001437 [00:14<00:02, 11092410.45it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 145424384/169001437 [00:14<00:02, 11123477.02it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 148537344/169001437 [00:15<00:01, 11162987.14it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 151683072/169001437 [00:15<00:01, 11235619.67it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 154796032/169001437 [00:15<00:01, 11263969.26it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 157908992/169001437 [00:15<00:00, 11338434.09it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 161054720/169001437 [00:16<00:00, 11253228.47it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 164200448/169001437 [00:16<00:00, 11227283.28it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 167313408/169001437 [00:16<00:00, 11244850.72it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169001437/169001437 [00:16<00:00, 10050370.33it/s]
Extracting ./data/cifar100/cifar-100-python.tar.gz to ./data/cifar100
Files already downloaded and verified
wandb: Currently logged in as: jashpatel8561 (maa_64). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /scratch/data/m22cs061/VIT_for_smallDB/ORG/wandb/run-20240930_213612-4055lv3c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run org_architecture_cifar100_Lr_3e-4_EMB_768_patch_16_depth_12
wandb: â­ï¸ View project at https://wandb.ai/maa_64/vit-small-data
wandb: ðŸš€ View run at https://wandb.ai/maa_64/vit-small-data/runs/4055lv3c
  0%|          | 0/100 [00:00<?, ?it/s]
Epoch: 1
Train Loss: 4.60896  Test Loss: 4.57544  ||  Train Accuray: 0.01206  Test Accuray: 0.01737
  1%|          | 1/100 [07:33<12:27:45, 453.19s/it]