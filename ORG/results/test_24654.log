Sun Aug 11 18:25:59 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:B7:00.0 Off |                    0 |
| N/A   46C    P0             57W /  400W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+


Experiment Name: org_architecture
Experiment Details: base vit architecture but Residual at only Multi head attention


Dataset Name: stl10
Seed: 64
Batch Size: 64
Number of Epochs: 100
Learning Rate: 3e-4
Input Channel: 3
Patch Size: 16
Embedding Size: 768
Input Image Size: 224
ViT Depth: 12
Number of Classes: 10
WandB Project: vit-small-data
WandB Run Name: org_architecture_stl10_Lr_3e-4_EMB_768_patch_16_depth_12
Output Directory: results


===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
├─PatchEmbedding: 1-1                         [-1, 197, 768]            --
|    └─Sequential: 2-1                        [-1, 196, 768]            --
|    |    └─Conv2d: 3-1                       [-1, 768, 14, 14]         590,592
|    |    └─Rearrange: 3-2                    [-1, 196, 768]            --
├─TransformerEncoder: 1-2                     [-1, 197, 768]            --
|    └─TransformerEncoderBlock: 2-2           [-1, 197, 768]            --
|    |    └─Residual: 3-3                     [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-4                   [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-3           [-1, 197, 768]            --
|    |    └─Residual: 3-5                     [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-6                   [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-4           [-1, 197, 768]            --
|    |    └─Residual: 3-7                     [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-8                   [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-5           [-1, 197, 768]            --
|    |    └─Residual: 3-9                     [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-10                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-6           [-1, 197, 768]            --
|    |    └─Residual: 3-11                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-12                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-7           [-1, 197, 768]            --
|    |    └─Residual: 3-13                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-14                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-8           [-1, 197, 768]            --
|    |    └─Residual: 3-15                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-16                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-9           [-1, 197, 768]            --
|    |    └─Residual: 3-17                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-18                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-10          [-1, 197, 768]            --
|    |    └─Residual: 3-19                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-20                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-11          [-1, 197, 768]            --
|    |    └─Residual: 3-21                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-22                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-12          [-1, 197, 768]            --
|    |    └─Residual: 3-23                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-24                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-13          [-1, 197, 768]            --
|    |    └─Residual: 3-25                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-26                  [-1, 197, 768]            4,723,968
├─ClassificationHead: 1-3                     [-1, 10]                  --
|    └─Reduce: 2-14                           [-1, 768]                 --
|    └─LayerNorm: 2-15                        [-1, 768]                 1,536
|    └─Linear: 2-16                           [-1, 10]                  7,690
===============================================================================================
Total params: 85,672,714
Trainable params: 85,672,714
Non-trainable params: 0
Total mult-adds (M): 456.65
===============================================================================================
Input size (MB): 0.57
Forward/backward pass size (MB): 15.01
Params size (MB): 326.82
Estimated Total Size (MB): 342.40
===============================================================================================

 ===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
├─PatchEmbedding: 1-1                         [-1, 197, 768]            --
|    └─Sequential: 2-1                        [-1, 196, 768]            --
|    |    └─Conv2d: 3-1                       [-1, 768, 14, 14]         590,592
|    |    └─Rearrange: 3-2                    [-1, 196, 768]            --
├─TransformerEncoder: 1-2                     [-1, 197, 768]            --
|    └─TransformerEncoderBlock: 2-2           [-1, 197, 768]            --
|    |    └─Residual: 3-3                     [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-4                   [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-3           [-1, 197, 768]            --
|    |    └─Residual: 3-5                     [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-6                   [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-4           [-1, 197, 768]            --
|    |    └─Residual: 3-7                     [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-8                   [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-5           [-1, 197, 768]            --
|    |    └─Residual: 3-9                     [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-10                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-6           [-1, 197, 768]            --
|    |    └─Residual: 3-11                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-12                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-7           [-1, 197, 768]            --
|    |    └─Residual: 3-13                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-14                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-8           [-1, 197, 768]            --
|    |    └─Residual: 3-15                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-16                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-9           [-1, 197, 768]            --
|    |    └─Residual: 3-17                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-18                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-10          [-1, 197, 768]            --
|    |    └─Residual: 3-19                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-20                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-11          [-1, 197, 768]            --
|    |    └─Residual: 3-21                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-22                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-12          [-1, 197, 768]            --
|    |    └─Residual: 3-23                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-24                  [-1, 197, 768]            4,723,968
|    └─TransformerEncoderBlock: 2-13          [-1, 197, 768]            --
|    |    └─Residual: 3-25                    [-1, 197, 768]            2,365,440
|    |    └─Sequential: 3-26                  [-1, 197, 768]            4,723,968
├─ClassificationHead: 1-3                     [-1, 10]                  --
|    └─Reduce: 2-14                           [-1, 768]                 --
|    └─LayerNorm: 2-15                        [-1, 768]                 1,536
|    └─Linear: 2-16                           [-1, 10]                  7,690
===============================================================================================
Total params: 85,672,714
Trainable params: 85,672,714
Non-trainable params: 0
Total mult-adds (M): 456.65
===============================================================================================
Input size (MB): 0.57
Forward/backward pass size (MB): 15.01
Params size (MB): 326.82
Estimated Total Size (MB): 342.40
=============================================================================================== 

with STL10
Files already downloaded and verified
Files already downloaded and verified


EXP org_architecture: Original VIT on stl10 with depth 12 and LEARNIGN_RATE 0.0003



wandb: Currently logged in as: jashpatel8561 (maa_64). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /scratch/data/m22cs061/VIT_for_smallDB/ORG/wandb/run-20240811_182616-02ol41xt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run org_architecture_stl10_Lr_3e-4_EMB_768_patch_16_depth_12
wandb: ⭐️ View project at https://wandb.ai/maa_64/vit-small-data
wandb: 🚀 View run at https://wandb.ai/maa_64/vit-small-data/runs/02ol41xt
  0%|          | 0/100 [00:00<?, ?it/s]
Epoch: 1
Train Loss: 2.31484  Test Loss: 2.30583  ||  Train Accuray: 0.09889  Test Accuray: 0.10000
  1%|          | 1/100 [01:10<1:56:09, 70.40s/it]
Epoch: 2
Train Loss: 2.31168  Test Loss: 2.31040  ||  Train Accuray: 0.10398  Test Accuray: 0.10000
  2%|▏         | 2/100 [02:20<1:55:05, 70.46s/it]
Epoch: 3
Train Loss: 2.31238  Test Loss: 2.30778  ||  Train Accuray: 0.10133  Test Accuray: 0.10000
  3%|▎         | 3/100 [03:31<1:53:46, 70.37s/it]
Epoch: 4
Train Loss: 2.31328  Test Loss: 2.30926  ||  Train Accuray: 0.10347  Test Accuray: 0.10000
  4%|▍         | 4/100 [04:41<1:52:27, 70.29s/it]
Epoch: 5
Train Loss: 2.31027  Test Loss: 2.31236  ||  Train Accuray: 0.10219  Test Accuray: 0.10000
  5%|▌         | 5/100 [05:51<1:51:14, 70.26s/it]
Epoch: 6
Train Loss: 2.30898  Test Loss: 2.31055  ||  Train Accuray: 0.10088  Test Accuray: 0.10000
  6%|▌         | 6/100 [07:01<1:50:01, 70.23s/it]
Epoch: 7
Train Loss: 2.31128  Test Loss: 2.31172  ||  Train Accuray: 0.10096  Test Accuray: 0.10000
  7%|▋         | 7/100 [08:12<1:48:56, 70.29s/it]
Epoch: 8
Train Loss: 2.31171  Test Loss: 2.30886  ||  Train Accuray: 0.09924  Test Accuray: 0.10000
  8%|▊         | 8/100 [09:22<1:47:47, 70.30s/it]
Epoch: 9
Train Loss: 2.31012  Test Loss: 2.30894  ||  Train Accuray: 0.09968  Test Accuray: 0.10000
  9%|▉         | 9/100 [10:32<1:46:34, 70.27s/it]
Epoch: 10
Train Loss: 2.31030  Test Loss: 2.31727  ||  Train Accuray: 0.09564  Test Accuray: 0.10000
 10%|█         | 10/100 [11:42<1:45:21, 70.24s/it]
Epoch: 11
Train Loss: 2.31028  Test Loss: 2.30588  ||  Train Accuray: 0.09977  Test Accuray: 0.10000
 11%|█         | 11/100 [12:52<1:44:09, 70.22s/it]
Epoch: 12
Train Loss: 2.31074  Test Loss: 2.30794  ||  Train Accuray: 0.09896  Test Accuray: 0.10000
 12%|█▏        | 12/100 [14:03<1:42:59, 70.23s/it]
Epoch: 13
Train Loss: 2.31071  Test Loss: 2.30669  ||  Train Accuray: 0.10158  Test Accuray: 0.10000
 13%|█▎        | 13/100 [15:13<1:41:51, 70.24s/it]
Epoch: 14
Train Loss: 2.30936  Test Loss: 2.31068  ||  Train Accuray: 0.09989  Test Accuray: 0.10000
 14%|█▍        | 14/100 [16:23<1:40:34, 70.17s/it]
Epoch: 15
Train Loss: 2.31055  Test Loss: 2.30506  ||  Train Accuray: 0.09678  Test Accuray: 0.10000
 15%|█▌        | 15/100 [17:33<1:39:21, 70.13s/it]
Epoch: 16
Train Loss: 2.30940  Test Loss: 2.30744  ||  Train Accuray: 0.10640  Test Accuray: 0.10000
 16%|█▌        | 16/100 [18:43<1:38:07, 70.09s/it]
Epoch: 17
Train Loss: 2.31039  Test Loss: 2.30981  ||  Train Accuray: 0.09965  Test Accuray: 0.10000
 17%|█▋        | 17/100 [19:53<1:36:57, 70.09s/it]
Epoch: 18
Train Loss: 2.31097  Test Loss: 2.31410  ||  Train Accuray: 0.10021  Test Accuray: 0.10000
 18%|█▊        | 18/100 [21:03<1:35:49, 70.11s/it]
Epoch: 19
Train Loss: 2.30933  Test Loss: 2.30829  ||  Train Accuray: 0.09862  Test Accuray: 0.10000
 19%|█▉        | 19/100 [22:13<1:34:38, 70.11s/it]
Epoch: 20
Train Loss: 2.30977  Test Loss: 2.30797  ||  Train Accuray: 0.09873  Test Accuray: 0.10000
 20%|██        | 20/100 [23:24<1:33:30, 70.13s/it]
Epoch: 21
Train Loss: 2.30913  Test Loss: 2.30774  ||  Train Accuray: 0.09899  Test Accuray: 0.10000
 21%|██        | 21/100 [24:34<1:32:23, 70.17s/it]
Epoch: 22
Train Loss: 2.31060  Test Loss: 2.30642  ||  Train Accuray: 0.10038  Test Accuray: 0.10000
 22%|██▏       | 22/100 [25:44<1:31:14, 70.18s/it]
Epoch: 23
Train Loss: 2.30897  Test Loss: 2.31182  ||  Train Accuray: 0.10321  Test Accuray: 0.10000
 23%|██▎       | 23/100 [26:54<1:30:05, 70.21s/it]
Epoch: 24
Train Loss: 2.31081  Test Loss: 2.30905  ||  Train Accuray: 0.09501  Test Accuray: 0.10000
 24%|██▍       | 24/100 [28:04<1:28:44, 70.05s/it]
Epoch: 25
Train Loss: 2.30910  Test Loss: 2.30467  ||  Train Accuray: 0.10096  Test Accuray: 0.10000
 25%|██▌       | 25/100 [29:14<1:27:28, 69.99s/it]
Epoch: 26
Train Loss: 2.30930  Test Loss: 2.31110  ||  Train Accuray: 0.09538  Test Accuray: 0.10000
 26%|██▌       | 26/100 [30:24<1:26:23, 70.05s/it]
Epoch: 27
Train Loss: 2.30926  Test Loss: 2.30648  ||  Train Accuray: 0.09948  Test Accuray: 0.09598
 27%|██▋       | 27/100 [31:34<1:25:16, 70.09s/it]slurmstepd: error: *** JOB 24654 ON gpu2 CANCELLED AT 2024-08-11T18:58:32 ***
